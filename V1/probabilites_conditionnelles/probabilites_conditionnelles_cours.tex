\documentclass{article}
% \usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
% \usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mdframed}
\usepackage{tikz}
\usepackage{tkz-tab}
\usetikzlibrary{shapes,backgrounds}
\usepackage{pgfplots}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{tabularx}
\usepackage{forest}
\usepackage[fleqn]{amsmath}
\setlength{\mathindent}{0pt}

% Spécifications du document
\newcommand{\doctitre}{Probabilités et variables aléatoires} % Ex: Le second degré
\newcommand{\docniveau}{$1^{\text{re}}$ et T$^{\text{le}}$ Spé math} % Ex: $1^{\text{re}}$ Spécialité mathématiques
\newcommand{\doctheme}{Probabilités et Statistiques} %Ex: Algèbre
\newcommand{\doctype}{Cours} % Ex: Démonstrations
\newcommand{\docshorttype}{Cours} % Démo

% Couleurs pour les graphiques
\definecolor{dark_green}{HTML}{008000}

% Paramètres du document
\RequirePackage{geometry}
\geometry{tmargin=1cm,bmargin=1.9cm,lmargin=1.9cm,rmargin=1.9cm}
% \renewcommand{\familydefault}{\sfdefault}
\setlength{\parindent}{0pt}
\title{\doctitre}
\author{\docniveau \\ \doctheme\text{ - }\doctype}
\date{}
\fancypagestyle{custom}{
  \fancyhf{}
  \renewcommand{\headrulewidth}{0pt}
  \lfoot{Probas et Stats - \docshorttype}
  \cfoot{\doctitre} % Change \titre to \doctitre
  \rfoot{\thepage/\pageref{LastPage}}
}

% Styles pour les mdframed
\mdfdefinestyle{definitionStyle}{
    leftline=true,
    rightline=false,
    topline=false,
    bottomline=false,
    linewidth=2pt,
    linecolor=black,
    innertopmargin=0pt,
    innerbottommargin=0pt,
    innerrightmargin=0pt,
    innerleftmargin=5pt,
}

\mdfdefinestyle{proprieteStyle}{
    linewidth=1pt,
    linecolor=black,
    innertopmargin=5pt,
    innerbottommargin=5pt,
    innerrightmargin=5pt,
    innerleftmargin=5pt,
}
% ----- DEBUT DU DOCUMENT -----
\begin{document}

% Style et numérotation
\maketitle
\pagestyle{custom}
\thispagestyle{custom}

\section{Vocabulaire et notations}

\subsection{Univers et évènement}

\begin{mdframed}[style=definitionStyle]
    \textbf{Définition :} ~\\
    On appelle univers l'ensemble des issues possibles d'une expérience aléatoire. On le note souvent $\Omega$.\\
    On appelle évènement un sous-ensemble de l'univers $\Omega$, c'est à dire un ensemble d'issues.
\end{mdframed}

\subsection{Inclusion}

\begin{mdframed}[style=definitionStyle]
    \textbf{Définition :} ~\\
    Pour deux ensembles $A$ et $B$, on dit que $A$ est inclus dans $B$ (ou que $A$ est un sous-ensemble de $B$) lorsque tous les éléments appartenant à $A$ 
    appartiennent aussi à $B$. On le note $A \subset B$.
\end{mdframed}

\textbf{Exemples :}
\vspace{-4pt}
\begin{itemize}
  \item $\{ 3;9 \} \subset \{ 3;6;9 \} $
  \item $\{ 1 \} \subset \{ 0;1;1;2;3;5 \} $
\end{itemize}

\subsection{Intersection et réunion}

\begin{mdframed}[style=definitionStyle]
    \textbf{Définitions :} ~\\
    Soit $\Omega$ un ensemble et $A$, $B$ deux sous-ensembles de $\Omega$.
    \begin{itemize}
      \item On appelle l'intersection de $A$ et $B$ l'ensemble des éléments de $\Omega$ qui appartiennent à la fois à
      $A$ et à $B$. On la note $A \cup B$.
      \item On appelle la réunion (ou l'union) de $A$ et $B$ l'ensemble des éléments de $\Omega$ qui appartiennent à au
      moins l'un des deux ensembles $A$ et $B$. On la note $A \cap B$.
    \end{itemize}
\end{mdframed}

\textbf{Exemples :}
\vspace{-4pt}
\begin{itemize}
  \item $ ]-\infty;3]\cap]2;+\infty[ \text{ $=$ } ]2;3]$
  \item $ [1;3] \cup ]2;+\infty[ \text{ $=$ } [1;+\infty[ $
\end{itemize}

\subsection{Complémentaire}

\begin{mdframed}[style=definitionStyle]
    \textbf{Définition :} ~\\
    Soit $\Omega$ un ensemble et $A$ un sous-ensemble de $\Omega$.\\
    On appelle complémentaire de $A$ dans $\Omega$ l'ensemble des éléments de $\Omega$ qui n'appartiennent pas à $A$. On le note $\bar{A}$
\end{mdframed}

\textbf{Exemple :} Soit $\Omega=\{ 1; 2; 3; 4; 5; 6 \}$ et $A=\{1; 3; 5; 6\}$. On a alors $\bar{A}=\{2; 4\}$

\subsection{Notations générales}

\begin{tabular}{|c|c|c|}
  \hline
  Notation & Vocabulaire ensembliste & Vocabulaire probabiliste \\
  \hline
  $\Omega$ & ensemble plein & évènement certain \\
  \hline
  $\emptyset$ & ensemble vide & évènement impossible \\
  \hline
  $\omega$ & élément de $\Omega$ & évènement élémentaire \\
  \hline
  $A$ & sous-ensemble de $\Omega$ & évènement \\
  \hline
  $\omega \in A$ & $\omega$ appartient à $A$ & $\omega$ réalise $A$ \\
  \hline
  $A \subset B$ & $A$ inclus dans $B$ & $A$ implique $B$ \\
  \hline
  $A \cup B$ & réunion de $A$ et $B$ & $A$ ou $B$ \\
  \hline
  $A \cap B$ & intersection de $A$ et $B$ & $A$ et $B$ \\
  \hline
  $\displaystyle \color{white}\frac{_0}{} \color{black}\bar{A} \color{white}\frac{_0}{}$ & complémentaire de $A$ & évènement contraire de $A$ \\
  \hline
  $A \cap B = \emptyset$ & $A$ et $B$ disjoints & $A$ et $B$ incompatibles \\
  \hline
\end{tabular}

\subsection{Modes de générations des ensembles}

\begin{mdframed}[style=definitionStyle]
    \textbf{Définition :} ~\\
    Lorsqu'on définit un ensemble en extension, on écrit la liste complête de ses éléments entre deux accolades. L'ordre et les répétitions ne sont pas pris en compte.
\end{mdframed}

\textbf{Exemple :} La notation $\{1; 2; 3\}$ désigne le même ensemble que la notation $\{1; 3; 2\}$ ou encore de la notation $\{1; 2; 2; 3\}$.
\vspace{4pt}
\begin{mdframed}[style=definitionStyle]
    \textbf{Définition :} ~\\
    Soit $E$ un ensemble. Lorsqu'on définit un sous-ensemble $F$ de $E$ en compréhension, on donne une proposition $P(x)$
    qui caractérise les éléments de $F$. \\
    L'ensemble des éléments de $E$ qui vérifient $P(x)$ est noté $\{x\in E \text{ }/\text{ } P(x)\}$.
\end{mdframed}

\textbf{Exemples :}
\begin{itemize}
  \item Soit $E=\{1; 2; 3; 4; 5\}$. La notation $\{x\in E \text{ }/\text{ } x \text{ est impair}\}$ désigne l'ensemble $\{1; 3; 5\}$.
  \item On définit en compréhension l'intervalle $[2, 5]$ par $\{ x\in\mathbb{R} \text{ }/\text{ } 2 \leq x \leq 5 \}$.
  \item En compréhension, l'ensemble des multiples de $3$ se note $\{ n\in\mathbb{Z} \text{ }/\text{ il existe un entier $k$ tel que } n=3k\}$
\end{itemize}

\subsection{Couples et produits cartésiens}

\begin{mdframed}[style=definitionStyle]
    \textbf{Définition :} ~\\
    Soient $x$ et $y$ deux objets (nombres, points\dots).\\
    On définit un nouveau type d'objet, que l'on note $(x,y)$ et que l'on appelle le couple $(x,y)$.
\end{mdframed}

\textbf{Remarque :} Deux couples $(x,y)$ et $(a,b)$ sont égaux si $x=a$ et $y=b$. Attention à ne pas confondre le couple
$(1,2)$ avec l'ensemble $\{1;2\}$ (qui lui, est égal à l'ensemble $\{ 2;1 \}$).

\begin{mdframed}[style=definitionStyle]
    \textbf{Définition :} ~\\
    Soient $E$ et $F$ deux ensembles. On appelle produit cartésien de $E$ et $F$ l'ensemble des couples $(x,y)$ avec
    $x\in E$ et $y\in F$. On le note $E\times F$.
\end{mdframed}

\textbf{Remarques :}
\begin{itemize}
  \item $E\times E = E^2$ et $E\times E\times E = E^3$.
  \item La notion de produit cartésien peut aussi s'étendre à plus de deux ensembles. Par exemple, si $E$, $F$ et $G$
  sont trois ensembles, le produit cartésien $E\times F\times G$ est l'ensemble des triplets $(x, y, z)$ avec $x\in E$,
  $y\in F$ et $z\in G$.
\end{itemize}

\textbf{Exemple :}
\begin{itemize}
  \item Soient $E=\{1;2\}$ et $F=\{ 7;8;9 \}$. On a alors $E\times F = \{ (1,7) ; (1,8) ; (1,9) ; (2,7) ; (2,8) ; (2,9) \}$.
  \item Soit $E=\{1;2\}$. On a alors $E^3 = \{  (1,1,1) ; (1,1,2) ; (1,2,1) ; (2,1,1) ; (1,2,2) ; (2,1,2) ; (2,2,1) ; (2,2,2)  \}$
\end{itemize}

\section{Probabilité conditionnelle et formule des probabilité totales}

\subsection{Probabilité d'un évènement}

\begin{mdframed}[style=definitionStyle]
    \textbf{Définition :} ~\\
    Soient $\Omega$ un unviers muni d'une loi de probabilité et $A$ un évènement non vide.\\
    On appelle probabilité de $A$ la somme des probabilités des issues appartenant à $A$. On la note $P(A)$.
\end{mdframed}

\subsection{Équiprobabilité}

\begin{mdframed}[style=definitionStyle]
    \textbf{Définition :} ~\\
    Dire que l'on fait l'hypothèse d'équiprobabilité sur l'univers $\Omega$ (ou que la loi de probabilité sur $\Omega$
    est uniforme) signifie que l'on associe la même probabilité à chaque issue.
\end{mdframed}

\subsection{Probabilité conditionnelle}

\begin{mdframed}[style=definitionStyle]
  \textbf{Définition :} ~\\
  Soient $A$ et $B$ deux évènements d'un univers $\Omega$ tels que $p(A)\not= 0$.
  La probabilité de $B$ sachant que $A$ est réalisé (ou de $B$ sachant $A$) est le nombre, noté $p_A(B)$ défini par $\displaystyle{}p_A(B)=\frac{p(A\cap B)}{p(A)}$.
\end{mdframed}

\begin{mdframed}[style=proprieteStyle]
  \textbf{Propriété :} ~\\
  La probabilité $p_A(B)$ vérifie bien $0\leq p_A(B)\leq1$ et $p_A(B)+p_A(\bar B)=1$.
\end{mdframed}

\subsection{Probabilité de l'intersection}

On en déduit une formule de calcul de la probabilité de l'intersection.

\begin{mdframed}[style=proprieteStyle]
  \textbf{Propriété :} ~\\
  Si $A$ et $B$ sont deux évènements avec $p(A)\not=0$ alors $p(A\cap B)=p_A(B)\times p(A)$.
\end{mdframed}

\textbf{Remarques :} 
\vspace{-3pt}
\begin{itemize}
  \item Si $p(B)\not=0$ alors on a aussi $p(A\cap B)=p_B(A)\times p(B)$.
  \item Dans toutes les formules, les rôles de $A$ et $B$ peuvent être inversés.
\end{itemize}

\subsection{Représentation à l'aide d'un arbre pondéré}

Pour modéliser une situation de probabilité conditionnelle, on utilise souvent un arbe pondéré : \\


\begin{forest}
  for tree={
  grow'=east,
  align=center,
  edge={thick},
  l sep+=2cm,
  s sep+=0.2cm
  }
  [
  [$A$, edge label={node[midway,above left]{$p(A)$}}
        [$B$, edge label={node[midway,above]{$p_A(B)$}}]
        [$\bar B$, edge label={node[midway,below]{$p_A(\bar B)$}}]
    ]
    [$\bar A$, edge label={node[midway,below left]{$p(\bar A)$}}
        [$B$, edge label={node[midway,above]{$p_{\bar A}(B)$}}]
        [$\bar B$, edge label={node[midway,below]{$p_{\bar A}(\bar B)$}}]
    ]
  ]
\end{forest}

\begin{mdframed}[style=proprieteStyle]
  \textbf{Propriétés \emph{(fonctionnement d'un arbre pondéré)} :}
  \begin{itemize}
    \item La somme des probabilités inscrites sur les branches partant d'un même nœud est égale à $1$.
    \item La probabilité d'un chemin est le produit des probabilités inscrites sur ses branches.
  \end{itemize}
\end{mdframed}

\textbf{Justifications :}

\begin{minipage}[t]{0.4\textwidth}
  \begin{itemize}
    \item $p(A)+p(\bar A)=1$
    \item $p_A(B)+p_A(\bar B)=1$
    \item $p_{\bar A}(B)+p_{\bar A}(\bar B)=1$
  \end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.6\textwidth}
  \begin{itemize}
    \item $p(A\cap B)=p_A(B)\times p(A)$
    \item $p(A\cap \bar B)=p_A(\bar B)\times p(A)$
    \item et ainsi de suite avec les autres chemins
  \end{itemize}
\end{minipage} ~\\

\textbf{Exemple :} ~\\
Dans un groupe de jeunes, $40\%$ sont des filles. Parmi les filles, il y a $30\%$ de skieuses. Parmi les garçons, il y a $50\%$ de skieurs.
Soit $F$ l'évènement « la personne est une fille » et soit $S$ l'évènement « la personne est fait du ski ».
\begin{equation*}
  \begin{split}
    \text{On a }&p(F)=40\%=0,4\\
    &p_F(S)=30\%=0,3 \\
    &p_{\bar{F}}(S)=50\%=0,5
  \end{split}
\end{equation*}

On calcule $p(F\cap S)=p(F)\times p_F(S)=0,4\times0,3=0,12$ \\
La probabilité de rencontrer une fille skieuse est de $0,12$. \\

On calcule $p(\bar F\cap S)=p(\bar F)\times p_{\bar{F}}(S)=0,6\times0,5=0,3$ \\
La probabilité de rencontrer un garçon skieur est de $0,3$.

\subsection{Partition d'un univers}

\begin{mdframed}[style=definitionStyle]
    \textbf{Définition :} ~\\
    Soit $\Omega$ un univers muni d'une loi de probabilité
    \vspace{-4pt}
    \begin{itemize}
      \item On dit que deux évènements $A_1$ et $A_2$ forment une partition de $\Omega$ si $A_1\cap A_2=\emptyset$ et $A_1\cup A_2=\Omega$.
      \item Plus généralement, on dit que des évènements forment une partition de $\Omega$ s'ils sont deux à deux
      incompatibles de leur réunion est l'univers tout entier.
    \end{itemize}
\end{mdframed}

\subsection{Formule des probabilités totales}

\begin{mdframed}[style=proprieteStyle]
  \textbf{Théorème :} ~\\
  Soit $A_1,A_2,\dots,A_n$ un système complet d'évènements de l'univers $\Omega$. \\
  Alors la probabilité d'un évènement quelconque $B$ est donné par :
  \vspace{-8pt}
  \begin{equation*}
    \begin{split}
      p(B)&=p(B\cap A_1)+p(B\cap A_2)+\dots+p(B\cap A_n)\\
      &= p(A_1)\times p_{A_1}(B)+p(A_2)\times p_{A_2}(B)+...+p(A_n)\times p_{A_n}(B)
    \end{split}
  \end{equation*}
\end{mdframed}

\textbf{Illustration du théorème avec un arbre pondéré :} ~\\

\begin{forest}
  for tree={
  grow'=east,
  align=center,
  edge={thick},
  l sep+=2cm,
  s sep+=0.1cm
  }
  [, coordinate % Transformer le nœud racine en un point de coordonnée
  [$A_1$, edge, name=A1
  [$B \quad \color{dark_green} p(A_1 \cap B)$, edge ]
  [$\bar B \quad \quad \quad \quad \quad$, edge ]
  ]
  [$A_2$, edge
    [$B \quad \color{dark_green} p(A_2 \cap B)$, edge ]
    [$\bar B\quad \quad \quad \quad \quad$, edge ]
  ]
  [$A_3$, edge
    [$B \quad \color{dark_green}p(A_3 \cap B)$, edge ]
    [$\bar B\quad \quad \quad \quad \quad$, edge ]
  ]
  [$A_4$, edge, name=A4
  [$B \quad \color{dark_green} p(A_4 \cap B)$, edge ]
  [$\bar B\quad \quad \quad \quad \quad$, edge ]
  ]
  ]
  \begin{scope}[overlay]
    \draw[decorate, decoration={brace, amplitude=12pt, raise=4pt}, thick, dark_green] ([xshift=5cm, yshift=0.8cm]A1-| A4.east) -- ([xshift=5cm, yshift=+0.3cm]A4.east) node[midway, right=20pt] {$\color{dark_green}=p(B)$};
  \end{scope}
\end{forest}

\begin{mdframed}[style=proprieteStyle]
  \textbf{Propriété \emph{(fonctionnement d'un arbre pondéré)} :} ~\\
  La probabilité d'un évènement correspondant à plusieurs chemins est la sommes des probabilités de ces chemins.
\end{mdframed}

\textbf{Exemple \emph{(suite)} :} ~\\
On cherche $p(S)=p(F\cap S)+p(\bar F\cap S)$ d'après la formule des probabilités totales et car $\{ F; \bar F \}$ forment un système complet d'évènements. On calcule $p(S)=0,12+0,3=0,42$.\\

On cherche $\displaystyle{}p_S(F)=\frac{p(S\cap F)}{p(S)}=\frac{0,12}{0,42}=\frac{12}{42}=\frac{2}{7}.$

\section{Indépendance de deux évènements}

\begin{mdframed}[style=definitionStyle]
  \textbf{Définition :} ~\\
  On dit que deux évènements $A$ et $B$ sont indépendants lorsque $p(A\cap B)=p(A)\times p(B)$.
\end{mdframed}

\begin{mdframed}[style=proprieteStyle]
  \textbf{Propriété :} ~\\
  On suppose que $p(A)\not=0$. $A$ et $B$ sont indépendants si et seulement si $p_A(B)=p(B)$.
\end{mdframed}

\textbf{Exemple :} ~\\
Une urne contient $3$ boules rouges numérotées $1$, $2$ et $3$ et $6$ boules noires numérotés $1$, $1$, $1$, $2$, $2$ et $3$. \\
Les boules sont indiscernables au toucher. \\

On tire une boule au hasard et on note :
\vspace{-6pt}
\begin{itemize}
  \item $R$ « tirer une boule rouge »
  \item $P$ « tirer une boule dont le numéro est pair »
  \item $U$ « tirer une boule dont le numéro est $1$ »
\end{itemize}

\begin{enumerate}
  \item Avec la première méthode : \\
  Calculons $p(R\cap P)=\frac{1}{9}$.\\
  D'autre part $p(R)=\frac{3}{9}=\frac{1}{3}$ et $p(P)=\frac{3}{9}=\frac{1}{3}$. \\
  On a $p(R\cap P)=p(R)\times p(P)$ donc les évènements $R$ et $P$ sont indépendants.
  \item Avec la deuxième méthode : \\
  Calculons $p_R(P)=\frac{1}{3}$.\\
  Or $p(P)=\frac{1}{3}$. \\
  On a $p_R(P)=p(P)$ donc les évènements $R$ et $P$ sont indépendants.
\end{enumerate}

\begin{mdframed}[style=proprieteStyle]
  \textbf{Propriété :} ~\\
  Si $A$ et $B$ sont deux évènements indépendants, alors $\bar A$ et $B$ le sont aussi.
\end{mdframed}

\section{Succession de deux épreuves indépendantes}

\begin{mdframed}[style=definitionStyle]
  \textbf{Définition :} ~\\
  Lorsque deux expériences aléatoires se succèdent et que les résultats de la première n'ont aucune influence sur les résultats de la seconde, ont dit qu'il s'agit d'une succession de deux épreuves indépendantes.
\end{mdframed}

\textbf{Exemple :} ~\\
On tire successivement deux cartes dans un jeu et on note les cartes obtenues.
\begin{itemize}
  \item Si on remet la carte dans le paquet après le premier tirage, les deux tirages sont indépendants.
  \item Si on ne remet pas la carte dans le paquet après le premier tirage, le contenu du paquet après le premier tirage dépend de la carte tirée en premier, donc les tirages ne sont pas indépendants.
\end{itemize}


\begin{mdframed}[style=proprieteStyle]
  \textbf{Propriété \emph{(admise)} :} ~\\
  Lorsque deux épreuves sont indépendantes, la probabilité d'un couple de résultat est égal au produit des probabilités de chacun d'entre eux.
\end{mdframed}

\textbf{Exemple :} ~\\
Un automobiliste rencontre deux feux tricolores. Ces deux feux fonctionnent de façon indépendante.
\begin{itemize}
  \item Le cycle du premier feu est : vert → 35s, orange → 5s et rouge → 20s.
  \item Le cycle du deuxième feu est : vert → 25s, orange → 5s et rouge → 30s.
\end{itemize}
Quelle est la probabilité que l'automobiliste croise un feu vert et un feu orange ? ~\\

On calcule $p(V\cap O)=\frac{35}{60}\times\frac{5}{60}+\frac{35}{60}\times\frac{25}{60}=\frac{1}{12}$.\\
La probabilité que l'automobiliste croise un feu vert et un feux orange est $\frac{1}{12}$.

\section{Variable aléatoire et loi de probabilité}

\subsection{Variable aléatoire discrète}

\begin{mdframed}[style=definitionStyle]
    \textbf{Définition :} ~\\
    Soit $\Omega=\{\omega_1;\omega_2;\dots;\omega_k\}$ l'univers associé à une expérience aléatoire. \\
    Une variable aléatoire $X$ sur l'univers $\Omega$ est une fonction définie sur $\Omega$ et à valeurs dans $\mathbb{R}$. \\
    Définir une variable aléatoire consiste à associer, à chaque issue $\omega_i$ de l'expérience aléatoire, un réel $x_i$. \\
    On note alors $\left(X=x_i\right)$ l'évènement formé des issues qui ont pour image $x_i$ par $X$.
\end{mdframed}

\textbf{Exemple :} ~\\
On lance un dé équilibré à 6 faces. \\
On gagne 2€ si le « 2 » sort, 1€ si le « 1 » sort et on perd 1€ dans tous les autres cas. \\
On appelle $X$ la variable aléatoire qui à chaque issue associe le gain obtenu. \\

On a $\Omega=\{1;2;3;4;5;6\}$ et $\Omega(X)=\{2;1;-1\}$. \\
On précise les évènements de $X$ :
\vspace{-4pt}
\begin{itemize}
    \item $(X=2)=\{2\}$
    \item $(X=1)=\{1\}$
    \item $(X=-1)=\{3;4;5;6\}$
\end{itemize}

\begin{mdframed}[style=definitionStyle]
    \textbf{Définition :} ~\\
    Soit $\Omega$ un univers et $X$ une variable aléatoire réelle sur $\Omega$.\\
    Pour tout réel $k$,
    \vspace*{-4pt}
    \begin{itemize}
      \item on note $(X\leq k)$ l'ensemble des issues dont l'image par $X$ est inférieur ou égal à k
      \item on note $(X\geq k)$ l'ensemble des issues dont l'image par $X$ est supérieur ou égal à k
    \end{itemize}
\end{mdframed}

\textbf{Exemple \emph{(suite)} :}
\vspace{-4pt}
\begin{itemize}
  \item $(X\leq2) = \{ 1;2 \}$
  \item $(X\geq2) = \{ 2;3;4;5;6 \}$
\end{itemize}

\subsection{Loi de probabilité}

\begin{mdframed}[style=definitionStyle]
    \textbf{Définition :} ~\\
    Soit $X$ une variable aléatoire qui prend les valeurs $\{x_1;x_2;\dots;x_k\}$. \\
    Donner la loi de probabilité de $X$, c'est donner la valeur $p(X=x_i)$, pour tout $i$ avec $1\leq i\leq k$.
    \vspace*{4pt}

    Les résultats sont généralement présentés sous forme d'un tableau : \\
    \vspace*{-8pt}

    \renewcommand{\arraystretch}{1.6}
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        Valeurs $x_i$ de $X$ & $x_1$      & $x_2$      & \quad\text{ }\dots\quad\text{ } & $x_k$      \\
        \hline
        Probabilité $p(X=x_i)$     & $p(X=x_1)$ & $p(X=x_2)$ & \quad\text{ }\dots\quad\text{ } & $p(X=x_k)$ \\
        \hline
    \end{tabular}
\end{mdframed}

\textbf{Remarque :} La somme des probabilités $p(X=x_i)$, pour $i$ allant de $1$ à $k$, est égal à $1$.\\

\textbf{Exemple \emph{(suite)} :} On donne la loi de probabilité de $X$.\\
\renewcommand{\arraystretch}{2}
\begin{tabular}{|l|c|c|c|c|}
    \hline
    Valeurs $x_i$ de $X$      & $2$                        & $1$                        & $-1$                                   \\
    \hline
    Probabilité  $p(X=x_i)\displaystyle{\color{white}\frac{1}{\frac{1}{1}}}$ & $\displaystyle\frac{1}{6}$ & $\displaystyle\frac{1}{6}$ & $\displaystyle\frac{4}{6}=\frac{2}{3}$ \\
    \hline
\end{tabular}

\section{Vocabulaire d'une variable aléatoire}

Soit $X$ une variable aléatoire qui prend en valeurs ${x_1;x_2;\dots;x_k}$ et dont la loi de probabilité est donnée par le tableau suivant :
\vspace{-12pt}

\begin{center}
    \renewcommand{\arraystretch}{1.6}
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        Valeurs $x_i$ de $X$ & $x_1$      & $x_2$      & \quad\text{ }\dots\quad\text{ } & $x_k$      \\
        \hline
        Probabilité $p(X=x_i)$      & $p_1$ & $p_2$ & \quad\text{ }\dots\quad\text{ } & $p_k$ \\
        \hline
    \end{tabular}
\end{center}

\subsection{Espérance}

\begin{mdframed}[style=definitionStyle]
    \textbf{Définition :} ~\\
    L'espérance de la variable aléatoire $X$ est le réel noté $E(X)$ définie par : \\
    $\displaystyle E(X)=\sum_{i=1}^{k}x_ip_i$          
\end{mdframed}

\textbf{Remarques :}
\begin{enumerate}
  \item L'espérance d'une variable aléatoire représente la valeur moyenne prise par $X$.
  \item Lorsque les valeurs prises par $X$ représentent les gains ou les pertes à un jeu, alors $E(X)$ représente le gain moyen par partie :
  \vspace*{-4pt}
  \begin{itemize}
      \item Si $E(X)>0$ alors le jeu est favorable au joueur.
      \item Si $E(X)<0$ alors le jeu est défavorable au joueur.
      \item Si $E(X)=0$ alors le jeu est équitable.
  \end{itemize}
\end{enumerate}

\subsection{Variance}

\begin{mdframed}[style=definitionStyle]
    \textbf{Définition :} ~\\
    La variance de la variable aléatoire $X$ est le réel noté $V(X)$ définie par :\\
    $\displaystyle V(X)=\sum_{i=1}^{k}\left[x_i-E(X)\right]^2\times p_i$
\end{mdframed}

\textbf{Remarque :} La variance d'une variable aléatoire $X$ se calcule aussi avec : $V(X)=\sum_{i=1}^{k} p_i x_i^2-E(X)^2$.
\subsection{Écart-type}

\begin{mdframed}[style=definitionStyle]
    \textbf{Définition :} ~\\
    L'écart-type $\sigma(X)$ est défini comme la racine carrée de la variance : $\sigma(X)=\sqrt{V(X)}$.
\end{mdframed}

\textbf{Remarque :} Par analogie avec les statistiques, de la même façon que $E(X)$ représente une moyenne, $V(X)$ et $\sigma(X)$ sont des indicateurs de dispersion des valeurs de $X$ autour de $E(X)$. \\
Plus la variance et l'écart-type sont grands, plus les valeurs sont dispersés autour de la moyenne (espérance). \\

\textbf{Exemple \emph{(suite)}:} ~\\
On calcule l'espérance de $X$ :
$\displaystyle E(X)=2\times\frac{1}{6}+1\times\frac{1}{6}-1\times\frac{2}{3}=-\frac{1}{6}\simeq-0,17$. \\
Sur un grand nombre de parties, le gain moyen par partie pour le joueur est $-0,17$. \\Donc le jeu n'est pas favorable au joueur. \\

On calcule la variance et l'écart-type :
\begin{itemize}
    \item $\displaystyle V(X)=\left[2-\left(-\frac{1}{6}\right)\right]^2\times \frac{1}{6}+\left[1-\left(-\frac{1}{6}\right)\right]^2\times \frac{1}{6}+\left[-1-\left(-\frac{1}{6}\right)\right]^2\times \frac{2}{3}=\frac{53}{36}\simeq1,47$
    \item $\displaystyle \sigma(X)=\sqrt{\frac{53}{36}}\simeq1,21$
\end{itemize}

\end{document}
% ----- FIN DU DOCUMENT -----